{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import csv\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel,AutoModelForSequenceClassification,AutoConfig, get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_NAME = \"roberta-base\"\n",
    "MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "MAX_LEN = 64\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32 \n",
    "LR = 2e-5 \n",
    "# WARMUP_STEPS = 100 \n",
    "# T_TOTAL = 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath, max_len):\n",
    "    label = []\n",
    "    sentences = []\n",
    "    # load dataset\n",
    "    f = open(filepath, 'r', encoding='utf-8')\n",
    "    r = csv.reader(f)\n",
    "    for item in r:\n",
    "        if r.line_num == 1:\n",
    "            continue\n",
    "        #print(item)\n",
    "    \n",
    "        label.append(int(item[2]))\n",
    "        sentences.append(item[1])\n",
    "        \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for data in sentences:\n",
    "        encoded_data = tokenizer.encode_plus(\n",
    "            text=data,                      # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=max_len,             # Max length to truncate/pad\n",
    "            padding='max_length',           # Pad sentence to max length\n",
    "            return_attention_mask=True,      # Return attention mask\n",
    "            truncation= True\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_data.get('input_ids'))\n",
    "        attention_masks.append(encoded_data.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    labels = torch.tensor(label)\n",
    "    return input_ids, attention_masks, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\Desktop\\\\SocialNetworkProj\\\\model'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset('train/train.csv', max_len = MAX_LEN)\n",
    "valid_dataset = load_dataset('train/val.csv', max_len = MAX_LEN)\n",
    "test_dataset = load_dataset('train/test.csv', max_len = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 0,  ..., 0, 2, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train_data = TensorDataset(train_dataset[0], train_dataset[1],train_dataset[2])\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size = BATCH_SIZE)\n",
    "\n",
    "val_data = TensorDataset(valid_dataset[0],valid_dataset[1],valid_dataset[2])\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_loader = DataLoader(val_data,sampler=val_sampler, batch_size = BATCH_SIZE)\n",
    "\n",
    "test_data = TensorDataset(test_dataset[0],test_dataset[1],test_dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(pre, label):\n",
    "    pre = pre.argmax(dim=1)\n",
    "    correct = torch.eq(pre, label).sum().float().item()\n",
    "    accuracy = correct / float(len(label))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "# config.num_labels = 3\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels = 3)\n",
    "model.cuda()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = LR)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 201it [01:20,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 | step:200 | avg_batch_acc:0.765858 | avg_batch_loss:0.560319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 401it [02:38,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 | step:400 | avg_batch_acc:0.767344 | avg_batch_loss:0.556397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 601it [03:56,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 | step:600 | avg_batch_acc:0.791562 | avg_batch_loss:0.510752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 687it [04:30,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 | step:686 | avg_batch_acc:0.796512 | avg_batch_loss:0.510198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 | avg_train_loss:0.538473016266566 | val_loss:0.5356500614867654 | val_accuracy:0.7994186046511628\n",
      "saving trained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 201it [01:18,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 | step:200 | avg_batch_acc:0.837065 | avg_batch_loss:0.420908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 401it [02:37,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 | step:400 | avg_batch_acc:0.835469 | avg_batch_loss:0.416487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 601it [03:54,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 | step:600 | avg_batch_acc:0.824063 | avg_batch_loss:0.448980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 687it [04:27,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 | step:686 | avg_batch_acc:0.839026 | avg_batch_loss:0.426822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 | avg_train_loss:0.42853377351681043 | val_loss:0.5138168627786082 | val_accuracy:0.806218853820598\n",
      "saving trained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 201it [01:17,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 | step:200 | avg_batch_acc:0.881996 | avg_batch_loss:0.323796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 401it [02:34,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 | step:400 | avg_batch_acc:0.868437 | avg_batch_loss:0.332743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 601it [03:52,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 | step:600 | avg_batch_acc:0.875469 | avg_batch_loss:0.334079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 687it [04:26,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 | step:686 | avg_batch_acc:0.881541 | avg_batch_loss:0.315840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 | avg_train_loss:0.32839812772156024 | val_loss:0.574549911153871 | val_accuracy:0.8004568106312292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 201it [01:18,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 | step:200 | avg_batch_acc:0.917600 | avg_batch_loss:0.235902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 401it [02:37,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 | step:400 | avg_batch_acc:0.915625 | avg_batch_loss:0.243981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 601it [03:55,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 | step:600 | avg_batch_acc:0.917969 | avg_batch_loss:0.238821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 687it [04:29,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 | step:686 | avg_batch_acc:0.909520 | avg_batch_loss:0.264463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 | avg_train_loss:0.24267895316387889 | val_loss:0.6276241961953252 | val_accuracy:0.8011835548172757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 201it [01:18,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 | step:200 | avg_batch_acc:0.947450 | avg_batch_loss:0.164868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 401it [02:37,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 | step:400 | avg_batch_acc:0.947344 | avg_batch_loss:0.162822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 601it [03:55,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 | step:600 | avg_batch_acc:0.942031 | avg_batch_loss:0.177046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 687it [04:29,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 | step:686 | avg_batch_acc:0.933866 | avg_batch_loss:0.192720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 | avg_train_loss:0.17130433814125637 | val_loss:0.7815324270794558 | val_accuracy:0.7971864617940199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 201it [01:18,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 | step:200 | avg_batch_acc:0.964397 | avg_batch_loss:0.110018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 401it [02:37,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 | step:400 | avg_batch_acc:0.958125 | avg_batch_loss:0.125746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 601it [03:55,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 | step:600 | avg_batch_acc:0.959688 | avg_batch_loss:0.139227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 687it [04:29,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 | step:686 | avg_batch_acc:0.963663 | avg_batch_loss:0.116856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 | avg_train_loss:0.12395617578604314 | val_loss:0.9361229580502177 | val_accuracy:0.7956810631229236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 201it [01:18,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 | step:200 | avg_batch_acc:0.974658 | avg_batch_loss:0.087821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 401it [02:37,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 | step:400 | avg_batch_acc:0.968437 | avg_batch_loss:0.103273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 601it [03:55,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 | step:600 | avg_batch_acc:0.969688 | avg_batch_loss:0.106919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 687it [04:28,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 | step:686 | avg_batch_acc:0.972384 | avg_batch_loss:0.094272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 | avg_train_loss:0.09868683270229571 | val_loss:1.087910391563593 | val_accuracy:0.7919954318936877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 201it [01:18,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 | step:200 | avg_batch_acc:0.978545 | avg_batch_loss:0.077363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 401it [02:37,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 | step:400 | avg_batch_acc:0.978594 | avg_batch_loss:0.079324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 601it [03:55,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 | step:600 | avg_batch_acc:0.972656 | avg_batch_loss:0.088720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 687it [04:29,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 | step:686 | avg_batch_acc:0.977834 | avg_batch_loss:0.076076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 | avg_train_loss:0.08107912474670455 | val_loss:1.1608602089244267 | val_accuracy:0.7910091362126245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 201it [01:18,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 | step:200 | avg_batch_acc:0.984297 | avg_batch_loss:0.060047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 401it [02:37,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 | step:400 | avg_batch_acc:0.985313 | avg_batch_loss:0.051239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 601it [03:55,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 | step:600 | avg_batch_acc:0.980313 | avg_batch_loss:0.073381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 687it [04:29,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 | step:686 | avg_batch_acc:0.982922 | avg_batch_loss:0.051682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 | avg_train_loss:0.06031756949456127 | val_loss:1.3060335652079693 | val_accuracy:0.7932412790697675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 201it [01:19,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 | step:200 | avg_batch_acc:0.985852 | avg_batch_loss:0.054313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 401it [02:38,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 | step:400 | avg_batch_acc:0.987500 | avg_batch_loss:0.042552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 601it [03:57,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 | step:600 | avg_batch_acc:0.988281 | avg_batch_loss:0.042901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 687it [04:31,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 | step:686 | avg_batch_acc:0.982195 | avg_batch_loss:0.063108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 | avg_train_loss:0.048667650760556104 | val_loss:1.3368132090152696 | val_accuracy:0.7950581395348837\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "print('training...')\n",
    "best_acc = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_t, batch_loss, batch_acc, batch_counts = 0, 0, 0, 0\n",
    "    \n",
    "    model.train()\n",
    "    for step,batch in tqdm(enumerate(train_loader), desc = \"Training\"):\n",
    "        batch_counts +=1\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        outputs = model(b_input_ids, b_attn_mask, labels=b_labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_t += loss.item()\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        acc = batch_accuracy(logits, b_labels)\n",
    "        batch_acc += acc\n",
    "        \n",
    "        if (step % 200 == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "            print(f'epoch:{epoch} | step:{step} | avg_batch_acc:{batch_acc/batch_counts:^.6f} | avg_batch_loss:{batch_loss/batch_counts:^.6f}')\n",
    "            batch_acc, batch_loss, batch_counts = 0, 0, 0\n",
    "        \n",
    "    avg_train_loss = loss_t / len(train_loader)\n",
    "    \n",
    "    #evaluate \n",
    "    val_acc, val_loss = [],[]\n",
    "    model.eval()\n",
    "    for batch in val_loader:\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, b_attn_mask, labels=b_labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        val_loss.append(loss.item())\n",
    "        acc = batch_accuracy(logits, b_labels)\n",
    "        val_acc.append(acc)\n",
    "        \n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_acc)\n",
    "            \n",
    "    print(f'epoch:{epoch} | avg_train_loss:{avg_train_loss} | val_loss:{val_loss} | val_accuracy:{val_accuracy}')\n",
    "    \n",
    "    if val_accuracy>=best_acc:\n",
    "        torch.save(model.state_dict(), 'bert_cla2.ckpt')\n",
    "        best_acc = val_accuracy\n",
    "        print('saving trained model...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始加载训练完成的model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('开始加载训练完成的model...')\n",
    "model.load_state_dict(torch.load('bert_cla2.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试...\n"
     ]
    }
   ],
   "source": [
    "print('开始测试...')\n",
    "model.eval()\n",
    "test_result = []\n",
    "for data in test_data:\n",
    "    b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in data)\n",
    "    b_input = b_input_ids.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input)\n",
    "        pre = outputs.logits.argmax(dim=1)\n",
    "        test_result.append([b_labels.item(), pre.item(), tokenizer.convert_ids_to_tokens(b_input_ids)])\n",
    "\n",
    "# 写入csv文件\n",
    "df = pd.DataFrame(test_result)\n",
    "df.to_csv('test_result.csv',index=False, header=['id', 'label','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('test_result.csv')\n",
    "df_e = df[df.id!=df.label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['&lt;s&gt;', 'the', 'Ġgr', 'ates', 'Ġchildren', 'Ġc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['&lt;s&gt;', 'going', 'Ġto', 'Ġsleep', 'Ġgonna', 'Ġ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['&lt;s&gt;', 'loo', 'ool', 'Ġme', 'Ġana', 'Ġi', 'Ġw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['&lt;s&gt;', 'oh', 'Ġno', 'Ġi', 'Ġhope', 'Ġyou', 'Ġ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['&lt;s&gt;', 'see', 'Ġyou', 'Ġon', 'Ġmay', '&lt;/s&gt;', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['&lt;s&gt;', 'its', 'Ġbout', 'Ġsmoking', 'Ġweed', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>['&lt;s&gt;', 'i', 'Ġthink', 'Ġthats', 'Ġpretty', 'Ġ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['&lt;s&gt;', 'went', 'Ġto', 'Ġsleep', 'Ġand', 'Ġthe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>['&lt;s&gt;', 'wh', 'ar', 'Ġa', 'Ġnight', 'Ġwoo', 'Ġ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['&lt;s&gt;', 'w', 'ish', 'Ġmy', 'Ġbeta', 'Ġkey', 'Ġ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2748 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label                                               text\n",
       "0      1      1  ['<s>', 'the', 'Ġgr', 'ates', 'Ġchildren', 'Ġc...\n",
       "1      0      0  ['<s>', 'going', 'Ġto', 'Ġsleep', 'Ġgonna', 'Ġ...\n",
       "2      1      1  ['<s>', 'loo', 'ool', 'Ġme', 'Ġana', 'Ġi', 'Ġw...\n",
       "3      2      1  ['<s>', 'oh', 'Ġno', 'Ġi', 'Ġhope', 'Ġyou', 'Ġ...\n",
       "4      1      1  ['<s>', 'see', 'Ġyou', 'Ġon', 'Ġmay', '</s>', ...\n",
       "...   ..    ...                                                ...\n",
       "2743   1      1  ['<s>', 'its', 'Ġbout', 'Ġsmoking', 'Ġweed', '...\n",
       "2744   2      2  ['<s>', 'i', 'Ġthink', 'Ġthats', 'Ġpretty', 'Ġ...\n",
       "2745   0      0  ['<s>', 'went', 'Ġto', 'Ġsleep', 'Ġand', 'Ġthe...\n",
       "2746   1      2  ['<s>', 'wh', 'ar', 'Ġa', 'Ġnight', 'Ġwoo', 'Ġ...\n",
       "2747   1      1  ['<s>', 'w', 'ish', 'Ġmy', 'Ġbeta', 'Ġkey', 'Ġ...\n",
       "\n",
       "[2748 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['&lt;s&gt;', 'oh', 'Ġno', 'Ġi', 'Ġhope', 'Ġyou', 'Ġ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['&lt;s&gt;', 'i', 'Ġhad', 'Ġit', 'Ġon', 'Ġmy', 'Ġit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>['&lt;s&gt;', 'i', 'Ġhope', 'Ġso', 'Ġrecorded', 'Ġth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['&lt;s&gt;', 'i', 'Ġfeel', 'Ġsome', 'Ġtype', 'Ġof',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['&lt;s&gt;', 'day', 'Ġof', 'Ġwork', 'Ġtoday', 'Ġwas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2722</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['&lt;s&gt;', 'my', 'Ġhub', 'by', 'Ġand', 'Ġhis', 'Ġ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2731</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['&lt;s&gt;', 'i', 'Ġcant', 'Ġfind', 'Ġmy', 'Ġtennis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2735</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>['&lt;s&gt;', 'thanks', 'Ġf', 'ot', 'ore', 'port', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>['&lt;s&gt;', 'y', 'ep', 'Ġthat', 'Ġone', 'Ġworks', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>['&lt;s&gt;', 'wh', 'ar', 'Ġa', 'Ġnight', 'Ġwoo', 'Ġ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>573 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label                                               text\n",
       "3      2      1  ['<s>', 'oh', 'Ġno', 'Ġi', 'Ġhope', 'Ġyou', 'Ġ...\n",
       "10     1      0  ['<s>', 'i', 'Ġhad', 'Ġit', 'Ġon', 'Ġmy', 'Ġit...\n",
       "14     0      2  ['<s>', 'i', 'Ġhope', 'Ġso', 'Ġrecorded', 'Ġth...\n",
       "18     1      0  ['<s>', 'i', 'Ġfeel', 'Ġsome', 'Ġtype', 'Ġof',...\n",
       "26     1      0  ['<s>', 'day', 'Ġof', 'Ġwork', 'Ġtoday', 'Ġwas...\n",
       "...   ..    ...                                                ...\n",
       "2722   2      1  ['<s>', 'my', 'Ġhub', 'by', 'Ġand', 'Ġhis', 'Ġ...\n",
       "2731   1      0  ['<s>', 'i', 'Ġcant', 'Ġfind', 'Ġmy', 'Ġtennis...\n",
       "2735   1      2  ['<s>', 'thanks', 'Ġf', 'ot', 'ore', 'port', '...\n",
       "2736   1      2  ['<s>', 'y', 'ep', 'Ġthat', 'Ġone', 'Ġworks', ...\n",
       "2746   1      2  ['<s>', 'wh', 'ar', 'Ġa', 'Ġnight', 'Ġwoo', 'Ġ...\n",
       "\n",
       "[573 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7914847161572053"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1- len(df_e)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce613df70ec087c2b4dda2bc280e25d341f72f59d81afb32edf1d298cbbb8087"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('bert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
